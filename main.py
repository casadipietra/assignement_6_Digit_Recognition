import pandas as pdimport numpy as npfrom homework import *from sklearn.decomposition import PCAfrom sklearn.decomposition import FastICAlabels=pd.read_csv("digits_labels.csv",header=None)#change 10 labels to 0s.for i in range(len(labels)):    if labels.iloc[i][0]==10:        labels.iloc[i]=0data=pd.read_csv("digits_data.csv",header=None)data['labels']=labelsdf=datanp.random.seed(8)training_set,testing_set=split_sample(df,0.8,permute=True)#Separate into X and YY_train,Y_test=training_set['labels'], testing_set['labels']X_train,X_test=training_set.iloc[:,:-1],testing_set.iloc[:,:-1]def accuracy(true_label,predicted):    summ=0    for i in range(len(true_label)):        if true_label[i]==predicted[i]:            summ+=1        return summ/len(true_label)            def sigmoid(x):    return 1/(1+np.exp(-x))def softmax(arr):    return [(np.exp(arr[i])/sum(np.exp(arr))) for i in range(len(arr))]def loss_func(y_hat,y,proba,weights,alpha,X_train):    #multi-class cross entropy loss function    loss=0    counter=0    bound=len(X_train[0])    for i in range(len(y_hat)):        if i>=bound:            bound+=len(X_train[0])            counter+=1        if y_hat[i]==y[i]:            #We add the L1 regularization here            #Alpha is its parameters            loss-=proba[int(y_hat[i][0])][0]+(alpha*np.abs(np.sum(weights[i-(counter*len(X_train[0]))])))    return loss    def run_neural_network(weights,X_train):    y_hat=np.zeros(shape=(len(X_train),1))    for entry in range(len(X_train)):            length_hidden_layer=10        layer_2=np.zeros(shape=(10,1))                for i in range(length_hidden_layer):            layer_2[i,0]=sigmoid(weights[:,i]@X_train[entry])        output_layer=softmax(layer_2)        result=output_layer==max(output_layer)        predict=[i  for i in range(len(output_layer)) if result[i]==True]        y_hat[entry,0]=predict[0]    return y_hat,output_layerdef gradient_descent(weights,Y_train,lr,X_train,epoch,alpha):    for _ in range(epoch):        y_hat,proba_list=run_neural_network(weights,X_train)        loss=loss_func(y_hat,Y_train,proba_list,weights,alpha,X_train)        x=weights.copy()        for i in range(len(weights)):            for j in range(len(weights[0])):                weights[i,j]=weights[i,j]-lr*((y_hat[i]*X_train[i,j])-(Y_train[i]*X_train[i,j]))    return weights,y_hatdef k_fold(train_sample,fold,alpha):    list_=[]    rang=[int(len(train_sample)/10)*i for i in range(1,(fold+1))]    for i in rang:        if i<=(len(train_sample)/10):            test=train_sample.iloc[:i]             train=train_sample.iloc[i:]        if i>(len(train_sample)/10):            test=train_sample.iloc[rang[rang.index(i)-1]:i]            train_sample1=train_sample.iloc[:i-int(len(train_sample)/10)]            train_sample2=train_sample.iloc[i:]            train=pd.concat([train_sample1,train_sample2])        train=np.array(train)        y_hat,proba_list=run_neural_network(weights,train)        loss=loss_func(y_hat,Y_train,proba_list,weights,alpha,train)        list_.append(loss)    return sum(list_)/len(list_)#- *Task 1* : Use the function the likeligood of the multinomial distribution discussed in Tutorial 4 to train the neural network. #Implement training with the standard and l1-regularized version of ERM based on multinomial loglikelihood. #Use GD or SGD techniques to train. #Use k-fold cross-validation (k up to your computational resources) to tune the strength of the l1-penalty.weights=np.random.rand(400,10)#Choosing the optimal l1 parameter with 10-fold lossbest_alpha=0best_loss=0for alpha in np.arange(0.1,2,0.1):    loss=k_fold(X_train,10,alpha)    if loss<best_loss:        best_loss=loss        best_alpha=alphaY_train,Y_test=np.array(training_set['labels']), np.array(testing_set['labels'])X_train,X_test=np.array(training_set.iloc[:,:-1]),np.array(testing_set.iloc[:,:-1])#Train with GDweights,y_hat=gradient_descent(weights,Y_train,0.1,X_train,500,best_alpha)#*Task 2* : Use the NN obtained to classify the digits in the training and testing samples.#Classify the training datay_hat_train,proba_list=run_neural_network(weights,X_train)#Classify the testing datay_hat_test,proba_list=run_neural_network(weights,X_test)print('\nTask 3\n')#*Task 3* : Create a misclassification matrix for the training and testing samples.def make_confusion_matrix(y_hat,y):    matrix=pd.DataFrame(index=[0,1,2,3,4,5,6,7,8,9],columns=[0,1,2,3,4,5,6,7,8,9])    matrix=matrix.fillna(0)    for i in range(len(y_hat)):        matrix.iloc[y[i],int(y_hat[i][0])]=matrix.iloc[y[i],int(y_hat[i][0])]+1    return matrixmatrix_train=make_confusion_matrix(y_hat_train, Y_train)print('The misclassification matrix on the training data: \n',matrix_train)matrix_test=make_confusion_matrix(y_hat_test, Y_test)print('The misclassification matrix on the training data: \n',matrix_test)# *Task 4* : Visualize the digits which have been mistakenly classified the most.#remove the correctly classified observationsmatrix_train_no_diagonal=matrix_train.copy()matrix_test_no_diagonal=matrix_test.copy()for loc1 in [0,1,2,3,4,5,6,7,8,9]:    for loc2 in [0,1,2,3,4,5,6,7,8,9]:        if loc1==loc2:            matrix_test_no_diagonal.iloc[loc1,loc2]=0            matrix_train_no_diagonal.iloc[loc1,loc2]=0#Look at max on misclassified for testingmost_misclass_test=matrix_test_no_diagonal[matrix_test_no_diagonal==matrix_test_no_diagonal.max().max()].max().dropna()print("\nThe most misclassified digits in the testing set are:\n ",most_misclass_test)#For trainingmost_misclass_train=matrix_train_no_diagonal[matrix_train_no_diagonal==matrix_train_no_diagonal.max().max()].max().dropna()print("\nThe most misclassified digits in the training set are:\n ",most_misclass_train)#Part 2#Using PCApca = PCA(n_components=10)principal_components_train = pca.fit_transform(X_train)principal_components_test = pca.fit_transform(X_test)#train the network#initializing random weightsweights=np.random.rand(10,10)#Choosing l1principal_components_train =pd.DataFrame(principal_components_train)best_alpha=0best_loss=0for alpha in np.arange(0.1,2,0.1):    loss=k_fold(principal_components_train,10,alpha)    if loss<best_loss:        best_loss=loss        best_alpha=alpha        principal_components_train =np.array(principal_components_train )weights,y_hat=gradient_descent(weights,Y_train,0.1,principal_components_train ,500,best_alpha)#Classify the training datay_hat_train_PCA,proba_list=run_neural_network(weights,principal_components_train)#Classify the testing datay_hat_test_PCA,proba_list=run_neural_network(weights,principal_components_test)#Misclassification matrixmatrix_train_PCA=make_confusion_matrix(y_hat_train_PCA, Y_train)print('\nThe misclassification matrix on the training data for PCA: \n',matrix_train_PCA)matrix_test_PCA=make_confusion_matrix(y_hat_test_PCA, Y_test)print('\nThe misclassification matrix on the training data for PCA: \n',matrix_test_PCA)#remove the correctly classified observationsmatrix_train_no_diagonal_PCA=matrix_train_PCA.copy()matrix_test_no_diagonal_PCA=matrix_test_PCA.copy()for loc1 in [0,1,2,3,4,5,6,7,8,9]:    for loc2 in [0,1,2,3,4,5,6,7,8,9]:        if loc1==loc2:            matrix_test_no_diagonal_PCA.iloc[loc1,loc2]=0            matrix_train_no_diagonal_PCA.iloc[loc1,loc2]=0#Look at max on misclassified for testingmost_misclass_test_PCA=matrix_test_no_diagonal_PCA[matrix_test_no_diagonal_PCA==matrix_test_no_diagonal_PCA.max().max()].max().dropna()print("\nThe most misclassified digits in the testing set for PCA are:\n ",most_misclass_test_PCA)#For trainingmost_misclass_train_PCA=matrix_train_no_diagonal_PCA[matrix_train_no_diagonal_PCA==matrix_train_no_diagonal_PCA.max().max()].max().dropna()print("\nThe most misclassified digits in the traing set for PCA are:\n ",most_misclass_train_PCA)#ICAICA= FastICA(n_components=10)ICA_train = ICA.fit_transform(X_train)ICA_test = ICA.fit_transform(X_test)#train the network#initializing random weightsweights=np.random.rand(10,10)#Choosing l1ICA_train =pd.DataFrame(ICA_train)best_alpha=0best_loss=0for alpha in np.arange(0.1,2,0.1):    loss=k_fold(ICA_train,10,alpha)    if loss<best_loss:        best_loss=loss        best_alpha=alpha        ICA_train =np.array(ICA_train)weights,y_hat=gradient_descent(weights,Y_train,0.1,ICA_train ,500,best_alpha)#Classify the training datay_hat_train_ICA,proba_list=run_neural_network(weights,ICA_train)#Classify the testing datay_hat_test_ICA,proba_list=run_neural_network(weights,ICA_test)#Misclassification matrixmatrix_train_ICA=make_confusion_matrix(y_hat_train_ICA, Y_train)print('\nThe misclassification matrix on the training data for ICA: \n',matrix_train_ICA)matrix_test_ICA=make_confusion_matrix(y_hat_test_ICA, Y_test)print('\nThe misclassification matrix on the training data for ICA: \n',matrix_test_ICA)#remove the correctly classified observationsmatrix_train_no_diagonal_ICA=matrix_train_ICA.copy()matrix_test_no_diagonal_ICA=matrix_test_ICA.copy()for loc1 in [0,1,2,3,4,5,6,7,8,9]:    for loc2 in [0,1,2,3,4,5,6,7,8,9]:        if loc1==loc2:            matrix_test_no_diagonal_ICA.iloc[loc1,loc2]=0            matrix_train_no_diagonal_ICA.iloc[loc1,loc2]=0#Look at max on misclassified for testing    most_misclass_test_ICA=matrix_test_no_diagonal_ICA[matrix_test_no_diagonal_ICA==matrix_test_no_diagonal_ICA.max().max()].max().dropna()print("\nThe most misclassified digits in the testing set for ICA are:\n ",most_misclass_test_ICA)#For trainingmost_misclass_train_ICA=matrix_train_no_diagonal_ICA[matrix_train_no_diagonal_ICA==matrix_train_no_diagonal_ICA.max().max()].max().dropna()print("\nThe most misclassified digits in the traing set for ICA are:\n ",most_misclass_train_ICA)
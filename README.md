<h1 align="center">
  <a>Assignment 6 ‚Äì Nonlinear learning models (Neural Networks) and data dimensionality reduction techniques</a>
</h1>
<h3 align="center">
  <a>Machine Learning @ Uni-Konstanz 2021</a>
</h3>

## Idea üìì

- Classify the MNIST handwritten digits dataset in the original representation with the NN lcassifier

- Apply linear and nonlinear data dimensionality reduction techniques to the MNIST handwritten digits dataset

- Classify the MNIST handwritten digits dataset in the dimensionality reduced representation with the NN lcassifier

***

**The goal of this exercise is creating a handwriting recognition for a data set that contains handwritten digits from 0 to 9** using a multiclass classifier generated by an artificial neural network with one hidden layer. The exercise is repeated on the data in the original 400-dimensional representation and on the dimensionality reduced one.

***

## Data üì¶

- `digits_data.csv`

- `digits_labels.csv`

The handwritten digits are characterized by a 20 x 20 pixel gray level matrix that is stored in the lines of the file `digits_data.csv`. The label corresponding to each digit is stored in `digits_labels.csv`; the digit "0" is mapped to label "10" in this list.

## Tasks üìù

#### Preliminaries

- You are given prepared functions in the previous tutorials. Others you will have to create.
	
#### Classification

For all the exercises split the data in `digits_data.csv` into training and testing with the ratio 80:20 (the sample is split randomly but keeping both the training and testing sets balanced in terms of representatives of different classes). 
	
**Part I: Implement the backpropagation algorithm and use the NN classifier on the original data**:
	
	- *Task 1* : Use the function the likeligood of the multinomial distribution discussed in Tutorial 4 to train the neural network. 
	Implement training with the standard and l1-regularized version of ERM based on multinomial loglikelihood. 
	Use GD or SGD techniques to train. 
	Use k-fold cross-validation (k up to your computational resources) to tune the strength of the l1-penalty.
	
	- *Task 2* : Use the NN obtained to classify the digits in the training and testing samples.
	
	- *Task 3* : Create a misclassification matrix for the training and testing samples.
	
	- *Task 4* : Visualize the digits which have been mistakenly classified the most.

**Part II: Implement the backpropagation algorithm**:
	
	- *Task 5* : Implement PCA (it is also in Tutorial 6) and any of the nonlinear dimensionality reduction techniques 
	(for the latter you can use built-in functions or the ML_toolbox, for Python users - any package) for the MNIST dataset. 
	In case some hyperparameters need to be chosen, use k-fold cross-validation (k up to your computational resources).
	
	- *Task 6* : Train the network and repeat the exercises in Part I for the dataset in its dimensionality reduced representation.
	
**Part III: Make conclusions**:	
	


<br>

***Work well!***

<br>
